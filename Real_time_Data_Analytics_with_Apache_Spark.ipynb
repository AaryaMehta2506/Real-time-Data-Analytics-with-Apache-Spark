{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Real time Data Analytics with Apache Spark***\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DdPNsgm24bTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plan\n",
        "\n",
        "Install Spark in Colab (Colab doesn't have it by default, but Java + setup is quick).\n",
        "\n",
        "Start SparkSession in local mode.\n",
        "\n",
        "Simulate streaming data by writing small CSV batches to a folder.\n",
        "\n",
        "Read the data with Spark Structured Streaming.\n",
        "\n",
        "Do real-time analytics (e.g., word count or transaction aggregation).\n",
        "\n",
        "Show output in the notebook."
      ],
      "metadata": {
        "id": "gr_BC4bM2_NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "an1FCczA164T"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, col\n",
        "import os\n",
        "import shutil\n",
        "import time"
      ],
      "metadata": {
        "id": "MgPxd9a84D2T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmNqOjH2qD77",
        "outputId": "6e07e613-9fbf-4d72-81e1-a4c760b5539c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Spark started successfully!\n",
            "✅ Streaming simulation finished!\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "# Start Spark Session\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RealTimeDataAnalytics\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "print(\"✅ Spark started successfully!\")\n",
        "\n",
        "# Create streaming input folder\n",
        "input_dir = \"/content/stream_data\"\n",
        "os.makedirs(input_dir, exist_ok=True)\n",
        "\n",
        "# Create schema and read streaming data\n",
        "from pyspark.sql.types import StructType, StringType\n",
        "\n",
        "schema = StructType().add(\"value\", StringType())\n",
        "\n",
        "stream_df = spark.readStream \\\n",
        "    .schema(schema) \\\n",
        "    .csv(input_dir)\n",
        "\n",
        "# Tokenize words\n",
        "words_df = stream_df.select(explode(split(stream_df.value, \" \")).alias(\"word\"))\n",
        "\n",
        "# Count word frequency\n",
        "word_count = words_df.groupBy(\"word\").count()\n",
        "\n",
        "# Start query to print results\n",
        "query = word_count.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# Simulate streaming by writing files\n",
        "import time\n",
        "sample_data = [\n",
        "    \"Apache Spark is awesome\",\n",
        "    \"Spark streaming with Python\",\n",
        "    \"Real time analytics is powerful\",\n",
        "    \"Data is the new oil\"\n",
        "]\n",
        "\n",
        "for line in sample_data:\n",
        "    with open(f\"{input_dir}/file_{int(time.time())}.csv\", \"w\") as f:\n",
        "        f.write(line)\n",
        "    time.sleep(2)  # Simulate delay\n",
        "\n",
        "query.awaitTermination(10)  # Run stream for 10 seconds\n",
        "query.stop()\n",
        "\n",
        "print(\"✅ Streaming simulation finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RealTimeDataAnalytics\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Create folder for streaming input\n",
        "input_dir = \"/tmp/stream_data\"\n",
        "if os.path.exists(input_dir):\n",
        "    shutil.rmtree(input_dir)\n",
        "os.makedirs(input_dir)\n",
        "\n",
        "# Read streaming data from CSV files\n",
        "stream_df = spark.readStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"sep\", \",\") \\\n",
        "    .schema(\"value STRING\") \\\n",
        "    .load(input_dir)\n",
        "\n",
        "# Split lines into words\n",
        "words = stream_df.select(\n",
        "    explode(split(col(\"value\"), \" \")).alias(\"word\")\n",
        ")\n",
        "\n",
        "# Count words\n",
        "word_count = words.groupBy(\"word\").count()\n",
        "\n",
        "# Write results to an in-memory table instead of console\n",
        "query = word_count.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"word_counts\") \\\n",
        "    .start()\n",
        "\n",
        "# Simulate streaming by writing files\n",
        "sample_data = [\n",
        "    \"Apache Spark is awesome\",\n",
        "    \"Spark streaming with Python\",\n",
        "    \"Real time analytics is powerful\",\n",
        "    \"Data is the new oil\"\n",
        "]\n",
        "\n",
        "for line in sample_data:\n",
        "    with open(f\"{input_dir}/file_{int(time.time())}.csv\", \"w\") as f:\n",
        "        f.write(line)\n",
        "    time.sleep(2)  # Simulate delay between incoming data\n",
        "\n",
        "# Wait for Spark to process\n",
        "time.sleep(5)\n",
        "\n",
        "# Show the results from the memory table\n",
        "spark.sql(\"SELECT * FROM word_counts ORDER BY count DESC\").show()\n",
        "\n",
        "# Stop the query\n",
        "query.stop()\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QH8Jz9m3-va",
        "outputId": "cdd743e1-42bd-4dec-85ce-1ee1f03cc26b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|word|count|\n",
            "+----+-----+\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ejazj7We4QZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}